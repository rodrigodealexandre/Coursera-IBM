{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the interface\n",
    "In this project, the goal is to create an interface that allows communication with a voice assistant, and a backend to manage the sending and receiving of responses.\n",
    "\n",
    "The frontend will use HTML, CSS and JavaScript with popular libraries such as Bootstrap for basic styling, Font Awesome for icons and JQuery for efficient handling of actions. The user interface will be similar to other voice assistant applications, like Google Assistant. The code for the interface is provided and the focus of the course is on building the voice assistant and integrating it with various services and APIs. The provided code will help you to understand how the frontend and backend interact, and as you go through it, you will learn about the important parts and how it works, giving you a good understanding of how the frontend works and how to create this simple web page.\n",
    "\n",
    "Run the following commands in the terminal to receive the outline of the project, rename it with another name and finally move into that directory:\n",
    "\n",
    "```\n",
    "git clone https://github.com/ibm-developer-skills-network/translator-with-voice-and-watsonx\n",
    "cd translator-with-voice-and-watsonx\n",
    "```\n",
    "\n",
    "HTML, CSS, and JavaScript\n",
    "The `index.html` file is responsible for the layout and structure of the web interface. This file contains the code for incorporating external libraries such as JQuery, Bootstrap, and FontAwesome Icons, as well as the CSS (`style.css`) and JavaScript code (`script.js`) that control the styling and interactivity of the interface.\n",
    "\n",
    "The `style.css` file is responsible for customizing the visual appearance of the page's components. It also handles the loading animation using CSS keyframes. Keyframes are a way of defining the values of an animation at various points in time, allowing for a smooth transition between different styles and creating dynamic animations.\n",
    "\n",
    "The `script.js` file is responsible for the page's interactivity and functionality. It contains the majority of the code and handles all the necessary functions such as switching between light and dark mode, sending messages, and displaying new messages on the screen. It even enables the users to record audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Understanding the server\n",
    "The server is how the application will run and communicate with all our services. Flask is a web development framework for Python and can be used as a backend for the application. It is a lightweight and simple framework that makes it quick and easy to build web applications.\n",
    "\n",
    "With Flask, you can create web pages and applications without requiring a lot of complex coding or using additional tools or libraries. You can create your own routes and handle user requests, and it also allows you to connect to external APIs and services to retrieve or send data.\n",
    "\n",
    "This guided project uses Flask to handle the backend of your voice assistant. This means that you will be using Flask to create routes and handle HTTP requests and responses. When a user interacts with the voice assistant through the frontend interface, the request will be sent to the Flask backend. Flask will then process the request and send it to the appropriate service.\n",
    "\n",
    "The code provided gives the outline for the server in the `server.py` file.\n",
    "\n",
    "At the top of the file, there are several import statements. These statements are used to bring in external libraries and modules, which will be used in the current file. For example, `speech_text` is a function inside the `worker.py` file, while `ibm_watson_machine_learning` is a package that needs to be installed to use Watsonx's flan-ul2 model. These imported packages, modules, and libraries will allow you to access the additional functionalities and methods that they offer, making it easy to interact with the speech-to-text and flan-ul2 models in your code.\n",
    "\n",
    "Underneath the imports, the Flask application is initialized, and a CORS policy is set. A CORS policy is used to allow or prevent web pages from making requests to different domains than the one that served the web page. Currently, it is set to * to allow any request.\n",
    "\n",
    "The server.py file consists of 3 functions which are defined as routes, and the code to start the server.\n",
    "\n",
    "Replace the first route in the `server.py` with the code below:\n",
    "\n",
    "```\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "```\n",
    "\n",
    "**Function explanation**\\\n",
    "When a user tries to load the application, they initially send a request to go to the `/` endpoint. They will then trigger this `index` function and execute the code above. Currently, the returned code from the function is a render function to show the `index.html` file which is the frontend interface.\n",
    "\n",
    "The second and third routes will be used to process all requests and handle sending information between the applications.\n",
    "\n",
    "Finally, the application is started with the `app.run` command to run on port `8000` and let the host be `0.0.0.0` (a.k.a. `localhost`).\n",
    "\n",
    "The next sections will take you through the process of completing the `process_message_route` and `speech_to_text_route` functions in this file and help you understand how to use the packages and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Running the application\n",
    "Docker allows for the creation of “containers” that package an application and its dependencies together. This allows the application to run consistently across different environments, as the container includes everything it needs to run. Additionally, using a Docker image to create and run applications can simplify the deployment process, as the image can be easily distributed and run on any machine that has Docker installed. This ensures that the application runs in the same way in development, testing, and production environments.\n",
    "\n",
    "The `git clone` from Step 1 already comes with a `Dockerfile` and `requirements.txt` for this application. These files are used to build the image with the dependencies already installed. Looking into the `Dockerfile` you can see it's simple, it creates a Python environment, moves all the files from the local directory to the container, installs the required packages, and then starts the application by running the `python` command.\n",
    "\n",
    "3 different containers need to run simultaneously to have the application run and interact with Text-to-Speech and Speech-to-Text capabilities.\n",
    "\n",
    "**Starting the application**\\\n",
    "This image is quick to build as the application is quite small. These commands first build the application (running the commands in the `Dockerfile`) and tag (names) the built container as `voice-translator-powered-by-watsonx`, then run it in the foreground on `port 8000`. You'll need to run these commands every time you wish to make a new change to one of the files.\n",
    "\n",
    "```\n",
    "docker build . -t voice-translator-powered-by-watsonx\n",
    "docker run -p 8000:8000 voice-translator-powered-by-watsonx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Integrating Watsonx API\n",
    "It's time to give your voice assistant a brain! With the power of Watsonx's API, we can pass the transcribed text and receive responses that answer your questions.\n",
    "\n",
    "**Authenticating for programmatic access**\\\n",
    "In this project, you do not need to specify your own `Watsonx_API` and `Project_id` to the below worker.py code. You can just specify `project_id=\"skills-network\"` and leave `Watsonx_API` blank, as in this CloudIDE environment, we have already granted you access to API without your own `Watsonx_API` and `Project_id`.\n",
    "\n",
    "**But it's important to note that this access method is exclusive to this Cloud IDE environment. If you are interested in using the model/API outside this environment (for example, in a local environment), detailed instructions and further information are available in this [tutorial]**(https://medium.com/the-power-of-ai/ibm-watsonx-ai-the-interface-and-api-e8e1c7227358).\n",
    "\n",
    "```\n",
    "# To call watsonx's LLM, we need to import the library of IBM Watson Machine Learning\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "\n",
    "# placeholder for Watsonx_API and Project_id incase you need to use the code outside this environment\n",
    "Watsonx_API = \"Your WatsonX API\"\n",
    "Project_id= \"Your Project ID\"\n",
    "\n",
    "# Define the credentials \n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "    #\"apikey\": 'API_KEY'\n",
    "}\n",
    "\n",
    "# Define the project id\n",
    "#project_id = \"PROJECT_ID\"\n",
    "project_id = \"skills-network\"\n",
    "    \n",
    "# Specify model_id that will be used for inferencing\n",
    "model_id = ModelTypes.FLAN_UL2\n",
    "\n",
    "# Define the model parameters\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 1024\n",
    "}\n",
    "\n",
    "# Define the LLM\n",
    "model = Model(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")\n",
    "```\n",
    "\n",
    "**Watsonx process message function**\\\n",
    "We will be updating the function called `watsonx_process_message`, which will take in a prompt and pass it to Watsonx's flan-ul2 API to receive a response. Essentially, it's the equivalent of pressing the send button to get a response from ChatGPT.\n",
    "\n",
    "Go ahead and update the `watsonx_process_message` function in the `worker.py` file with the following.\n",
    "```\n",
    "def watsonx_process_message(user_message):\n",
    "    # Set the prompt for Watsonx API\n",
    "    prompt = f\"\"\"Respond to the query: ```{user_message}```\"\"\"\n",
    "    response_text = model.generate_text(prompt=prompt)\n",
    "    print(\"wastonx response:\", response_text)\n",
    "    return response_text\n",
    "```\n",
    "\n",
    "**Prompt refinement**\\\n",
    "We can further optimize our translation assistant. Since this is a translator, users shouldn't have to type \"translate\" every time. To address this, we've improved the prompt in the watsonx_process_message function to be more explicit.\n",
    "\n",
    "For example, we now focus on translating sentences from English into Spanish, the updated prompt will look like below. Replace the prompt in the function with this:\n",
    "\n",
    "```\n",
    "prompt = f\"\"\"You are an assistant helping translate sentences from English into Spanish.\n",
    "    Translate the query to Spanish: ```{user_message}```.\"\"\"\n",
    "```\n",
    "\n",
    "This revised prompt makes it evident that the user intends to translate a sentence into Spanish, eliminating the need to explicitly mention \"translate.\"\n",
    "\n",
    "If your translation needs to involve languages other than Spanish, you can easily adapt the prompt. Simply replace \"Spanish\" in the prompt with the name of your required target language. This modification simplifies the user interaction and ensures that the translator remains user-friendly for various language pairs.\n",
    "\n",
    "**Function explanation**\\\n",
    "The function is really simple, thanks to the very easy-to-use `ibm_watson_machine_learning` library.\n",
    "\n",
    "Then we call Wastonx's API by using `model.generate_text` function and pass the prompt that we need the response for. Remember that `model` refers to the LLM we established earlier.\n",
    "\n",
    "Again, you can tweak these parameters according to your personalized needs and can learn more about them by going to IBM Wastonx Prompt Lab where you can test all parameters in real-time like below:\n",
    "![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0PPIEN/params.gif)\n",
    "\n",
    "Finally, we return the `response_text` which stores the answer to our prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Integrating Watson Speech-to-Text\n",
    "Speech-to-Text functionality is a technology that converts speech into text using machine learning. It is useful for accessibility, productivity, convenience, multilingual support, and cost-effective solutions for a wide range of applications. For example, being able to take a user's voice as input for a chat application.\n",
    "\n",
    "Using the embedded Watson Speech-to-Text AI model that was deployed earlier, it is possible to easily convert our Speech-to-Text by a simple API. This result can then be passed to Watsonx API for generating a response.\n",
    "\n",
    "Starting Speech-to-Text\n",
    "Skills Network provides its own Watson Speech-to-Text image that is run automatically in this environment. To access it, use this endpoint URL: `https://sn-watson-stt.labs.skills.network`\n",
    "\n",
    "You can test it works by running this query:\n",
    "\n",
    "`curl https://sn-watson-stt.labs.skills.network/speech-to-text/api/v1/models`\n",
    "\n",
    "You should see a list of a few languages it can recognize. An example output is shown below.\n",
    "\n",
    "```\n",
    "{\n",
    "   \"models\": [\n",
    "       {\n",
    "         \"name\": \"es-LA_Telephony\",\n",
    "         \"language\": \"es-LA\",\n",
    "         \"description\": \"Latin American Spanish telephony model for narrowband audio (8kHz)\",\n",
    "          ...\n",
    "      },\n",
    "      {\n",
    "         \"name\": \"en-US_Multimedia\",\n",
    "         \"language\": \"en-US\",\n",
    "         \"description\": \"US English multimedia model for broadband audio (16kHz or more)\",\n",
    "          ...\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "Next, try getting an example audio file to send a `/recognize` request to test the service. For example, you can download the example audio file by this command:\n",
    "\n",
    "`curl \"https://github.com/watson-developer-cloud/doc-tutorial-downloads/raw/master/speech-to-text/0001.flac\" -sLo example.flac`\n",
    "\n",
    "Send the audio file to the service:\n",
    "\n",
    "`curl \"https://sn-watson-stt.labs.skills.network/speech-to-text/api/v1/recognize\" --header \"Content-Type: audio/flac\" --data-binary @example.flac`\n",
    "\n",
    "Example response:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"result_index\": 0,\n",
    "   \"results\": [\n",
    "      {\n",
    "         \"final\": true,\n",
    "         \"alternatives\": [\n",
    "            {\n",
    "               \"transcript\": \"several tornadoes touched down as a line of severe thunderstorms swept through colorado on sunday \",\n",
    "               \"confidence\": 0.99\n",
    "            }\n",
    "         ]\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "To use a different model, add the `model` query parameter to the request. The audio format can also be changed as long as the `Content-Type` header matches. For example:\n",
    "\n",
    "```\n",
    "curl \"https://sn-watson-stt.labs.skills.network/speech-to-text/api/v1/recognize?model=es-LA_Telephony\" --header \"Content-Type: audio/flac\" --data-binary @example.flac\n",
    "\n",
    "{\n",
    "   \"result_index\": 0,\n",
    "   \"results\": [\n",
    "      {\n",
    "         \"final\": true,\n",
    "         \"alternatives\": [\n",
    "            {\n",
    "               \"transcript\": \"s \",\n",
    "               \"confidence\": 0.39\n",
    "            }\n",
    "         ]\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\\\n",
    "We will be updating a function called `speech_to_textin` the `worker.py` file that will take in audio data received from the browser and pass it to the Watson Speech-to-Text API.\n",
    "\n",
    "The `speech_to_text` function will take in audio data as a parameter, make an API call to the Watson Speech-to-Text API using the requests library, and return the transcription of the audio data.\n",
    "\n",
    "Remember to replace the `...` for the `base_url` variable with the URL for your Speech-to-Text model (for example,https://sn-watson-stt.labs.skills.network).\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "def speech_to_text(audio_binary):\n",
    "\n",
    "    # Set up Watson Speech-to-Text HTTP Api url\n",
    "    base_url = '...'\n",
    "    api_url = base_url+'/speech-to-text/api/v1/recognize'\n",
    "\n",
    "    # Set up parameters for our HTTP reqeust\n",
    "    params = {\n",
    "        'model': 'en-US_Multimedia',\n",
    "    }\n",
    "\n",
    "    # Set up the body of our HTTP request\n",
    "    body = audio_binary\n",
    "\n",
    "    # Send a HTTP Post request\n",
    "    response = requests.post(api_url, params=params, data=audio_binary).json()\n",
    "\n",
    "    # Parse the response to get our transcribed text\n",
    "    text = 'null'\n",
    "    while bool(response.get('results')):\n",
    "        print('Speech-to-Text response:', response)\n",
    "        text = response.get('results').pop().get('alternatives').pop().get('transcript')\n",
    "        print('recognised text: ', text)\n",
    "        return text\n",
    "```\n",
    "\n",
    "**Function explanation**\\\n",
    "The requests library imported at the top of our `worker.py` file is a simple HTTP request library that we will be using to make API calls to the Watson Speech-to-Text API.\n",
    "\n",
    "The function simply takes `audio_binary` as the only parameter and then sends it in the body of the HTTP request.\n",
    "\n",
    "To make an HTTP Post request to Watson Speech-to-Text API, we need the following three elements:\n",
    "\n",
    "* **URL** of the API: This is defined as `api_url` in our code and points to Watson's Speech-to-Text service\n",
    "* **Parameters**: This is defined as `params` in our code. It's just a dictionary having one key-value pair i.e. `'model': 'en-US_Multimedia'` which tells Watson that we want to use the US English model for processing our speech\n",
    "* **Body** of the request: this is defined as `body` and is equal to `audio_binary` since we are sending the audio data inside the body of our POST request.\n",
    "\n",
    "We then use the requests library to send this HTTP request passing in the URL, params, and data(body) to it and then use `.json()` to convert the API's response to json format which is very easy to parse and can be treated like a dictionary in Python.\n",
    "\n",
    "The structure of the response is something like this:\n",
    "```\n",
    "{\n",
    "  \"response\": {\n",
    "    \"results\": {\n",
    "      \"alternatives\": {\n",
    "        \"transcript\": \"Recognised text from your speech\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Therefore, we check if the response contains any results, and if it does, we extract the text by getting the nested transcript string as shown above. Then return this text.\n",
    "\n",
    "*Small tip\n",
    "Notice the print statements such as print(‘response’, response), it's always a good idea to print out the data you are receiving from some external place like an API in this case, as it really helps with debugging if something goes wrong*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Integrating Watson Text-to-Speech\n",
    "Time to give your assistant a voice using Text-to-Speech functionality.\n",
    "\n",
    "Once we have processed the user's message using Watsonx, let's add the final worker function that will convert that response to speech, so you get a more personalized feel as the Personal Assistant is going to read out the response to you. Just like other virtual assistants like Google, Alexa, Siri, etc.\n",
    "\n",
    "**Starting Text-to-Speech**\\\n",
    "Skills Network provides its own Watson Text-to-Speech image that is run automatically in this environment. To access it, use this endpoint URL: `https://sn-watson-tts.labs.skills.network`\n",
    "\n",
    "You can test it works by running this query:\n",
    "\n",
    "`curl https://sn-watson-tts.labs.skills.network/text-to-speech/api/v1/voices`\n",
    "\n",
    "You should see a list of a bunch of different voices this model can use. An example output is shown below.\n",
    "\n",
    "```\n",
    "{\n",
    "   \"voices\": [\n",
    "      {\n",
    "         \"name\": \"en-US_OliviaV3Voice\",\n",
    "         \"language\": \"en-US\",\n",
    "         \"gender\": \"female\",\n",
    "         \"description\": \"Olivia: American English female voice. Dnn technology.\",\n",
    "         ...\n",
    "      },\n",
    "      {\n",
    "         \"name\": \"es-ES_EnriqueV3Voice\",\n",
    "         \"language\": \"en-GB\",\n",
    "         \"gender\": \"male\",\n",
    "         \"description\": \"Enrique: Castilian Spanish (español castellano) male voice. Dnn technology.\",\n",
    "         ...\n",
    "      },\n",
    "      ...\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "Next, try sending an example text (ex: \"Hello world\") in JSON format to invoke `/synthesize` request. It will return an audio file named \"output.wav\" in the \"translator-with-voice-and-watsonx\" directory:\n",
    "\n",
    "`curl \"https://sn-watson-stt.labs.skills.network/text-to-speech/api/v1/synthesize\" --header \"Content-Type: application/json\" --data '{\"text\":\"Hello world\"}' --header \"Accept: audio/wav\" --output output.wav`\n",
    "\n",
    "To use a different model, add the `voice` query parameter to the request. To change the audio format, change the `Accept` header. For example:\n",
    "\n",
    "`curl \"https://sn-watson-stt.labs.skills.network/text-to-speech/api/v1/synthesize?voice=es-LA_SofiaV3Voice\" --header \"Content-Type: application/json\" --data '{\"text\":\"Hola! Hoy es un dia muy bonito.\"}' --header \"Accept: audio/mp3\" --output hola.mp3`\n",
    "\n",
    "After executing the above command, you'll find the output file named \"hola.mp3\" in the \"translator-with-voice-and-watsonx\" directory.\n",
    "\n",
    "**Text-to-Speech function**\n",
    "In the `worker.py` file, the `text_to_speech` function passes data to Watson's Text-to-Speech API to get the data as spoken output.\n",
    "\n",
    "This function is going to be similar to `speech_to_text` as we will be utilizing our request library again to make an HTTP request. Lets dive into the code. Again, remember to replace the `...` for the `base_url` variable with the URL for your Text-to-Speech model (for example, `https://sn-watson-tts.labs.skills.network`).\n",
    "\n",
    "**Function explanation**\\\n",
    "The function takes `text` and `voice` as parameters. It adds voice as a parameter to the `api_url` if it's not empty or not default. It sends the `text` in the body of the HTTP request.\n",
    "\n",
    "Similarly as before, to make an HTTP Post request to Watson Text-to-Speech API, we need the following three elements:\n",
    "\n",
    "* **URL** of the API: This is defined as `api_url` in our code and points to Watson's Text-to-Speech service. This time we also append a voice parameter to the `api_url` if the user has sent a preferred voice in their request.\n",
    "* **Headers**: This is defined as `headers` in our code. It's just a dictionary having two key-value pairs. The first is 'Accept':'audio/wav' which tells Watson that we are sending audio having wav format. The second one is 'Content-Type':'application/json', which means that the format of the body would be JSON\n",
    "* **Body** of the request: This is defined as `json_data` and is a dictionary containing 'text':`text` key-value pair, this text will then be processed and converted to a speech.\n",
    "\n",
    "We then use the requests library to send this HTTP to request passing in the URL, headers, and json(body) to it and then use `.json()` to convert the API's response to json format so we can parse it.\n",
    "\n",
    "The structure of the response is something like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"response\": {\n",
    "        content: The Audio data for the processed Text-to-Speech\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Therefore, we return `response.content` which contains the audio data received.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Putting everything together by creating Flask API endpoints\n",
    "Now by using the functions we defined in the previous sections, we can connect everything and complete the assistant.\n",
    "\n",
    "The changes in this section will be for the `server.py` file.\n",
    "\n",
    "The outline has already taken care of the imports for the functions from the `worker.py` file to the `server.py` file. This allows the `server.py` file to access these imported functions from the `worker.py` file.\n",
    "\n",
    "`from worker import speech_to_text, text_to_speech, watsonx_process_message`\n",
    "\n",
    "Now we will be updating two Flask routes, one for converting the user's Speech-to-Text (`speech_to_text_route`) and the other for processing their message and converting the Watsonx's response back to speech (`process_message_route`).\n",
    "\n",
    "**Speech-to-Text route**\\\n",
    "This function is simple, as it converts the user's Speech-to-Text using the `speech_to_text` we defined in one of our previous sections and returns the response. Replace the `speech_to_text_route` function with the code below:\n",
    "\n",
    "```\n",
    "@app.route('/speech-to-text', methods=['POST'])\n",
    "def speech_to_text_route():\n",
    "    print(\"processing Speech-to-Text\")\n",
    "    audio_binary = request.data # Get the user's speech from their request\n",
    "    text = speech_to_text(audio_binary) # Call speech_to_text function to transcribe the speech\n",
    "\n",
    "    # Return the response to user in JSON format\n",
    "    response = app.response_class(\n",
    "        response=json.dumps({'text': text}),\n",
    "        status=200,\n",
    "        mimetype='application/json'\n",
    "    )\n",
    "    print(response)\n",
    "    print(response.data)\n",
    "    return response\n",
    "```\n",
    "\n",
    "**Function explanation**\\\n",
    "We start by storing the `request.data` in a variable called `audio_binary`, as we are sending the binary data of audio in the body of the request from the frontend. Then we use our previously defined function `speech_to_text` and pass in the `audio_binary` as a parameter to it. We store the return value in a new variable called text.\n",
    "\n",
    "As our frontend expects a JSON response, we create a json response by using the Flask's `app.response_class` function and passing in three arguments:\n",
    "\n",
    "1. **response**: This is the actual data that we want to send in the body of our HTTP response. We will be using `json.dumps` function and will pass in a simple dictionary containing only one key-value pair -`'text': text`\n",
    "2. **status**: This is the status code of the HTTP response; we will set it to 200 which essentially means the response is OK and that the request has succeeded.\n",
    "3. **mimetype**: This is the format of our response which is more formally written as `'application/json'` in HTTP request/response.\n",
    "\n",
    "We then return the response.\n",
    "\n",
    "**Process message route**\n",
    "This function will basically accept a user's message in text form with their preferred voice. It will then use our previously defined helper functions to call the Watsonx's API to process this prompt and then finally convert that response to text using Watson's Text-to-Speech API and then return this data back to the user. Replace the `process_message_route` function to the code below:\n",
    "\n",
    "```\n",
    "@app.route('/process-message', methods=['POST'])\n",
    "def process_message_route():\n",
    "    user_message = request.json['userMessage'] # Get user's message from their request\n",
    "    print('user_message', user_message)\n",
    "\n",
    "    voice = request.json['voice'] # Get user\\'s preferred voice from their request\n",
    "    print('voice', voice)\n",
    "\n",
    "    # Call watsonx_process_message function to process the user's message and get a response back\n",
    "    watsonx_response_text = watsonx_process_message(user_message)\n",
    "\n",
    "    # Clean the response to remove any emptylines\n",
    "    watsonx_response_text = os.linesep.join([s for s in watsonx_response_text.splitlines() if s])\n",
    "\n",
    "    # Call our text_to_speech function to convert Watsonx Api's reponse to speech\n",
    "    watsonx_response_speech = text_to_speech(watsonx_response_text, voice)\n",
    "\n",
    "    # convert watsonx_response_speech to base64 string so it can be sent back in the JSON response\n",
    "    watsonx_response_speech = base64.b64encode(watsonx_response_speech).decode('utf-8')\n",
    "\n",
    "    # Send a JSON response back to the user containing their message\\'s response both in text and speech formats\n",
    "    response = app.response_class(\n",
    "        response=json.dumps({\"watsonxResponseText\": watsonx_response_text, \"watsonxResponseSpeech\": watsonx_response_speech}),\n",
    "        status=200,\n",
    "        mimetype='application/json'\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    return response\n",
    "```\n",
    "\n",
    "**Function explanation**\\\n",
    "We will start by storing the user's message in `user_message` by using `request.json['userMessage']`. Similarly, we will also store the user's preferred voice in `voice` by using `request.json['voice']`.\n",
    "\n",
    "We will then use the helper function we defined earlier to process this user's message by calling `watsonx_process_message(user_message)` and storing the response in `watsonx_response_text`. We will then clean this response to remove any empty lines by using a simple one-liner function in Python that is, `os.linesep.join([s for s in watsonx_response_text.splitlines() if s])`.\n",
    "\n",
    "Once we have this response cleaned, we will now use another helper function we defined earlier to convert it to speech. Therefore, we will call `text_to_speech` and pass in the two required parameters which are `watsonx_response_text` and `voice`. We will store the function's return value in a variable called `watsonx_response_speech`.\n",
    "\n",
    "As the `watsonx_response_speech` is a type of audio data, we can't directly send this inside a json as it can only store textual data. Therefore, we will be using something called \"base64 encoding\". We can convert any type of binary data to a textual representation by encoding the data in base64 format. Hence, we will simply use `base64.b64encode(watsonx_response_speech).decode('utf-8')` and store the result back to `watsonx_response_speech`.\n",
    "\n",
    "Now we have everything ready for our response so finally we will be using the same app.`response_class` function and send in the three parameters required. The `status` and `mimetype` will be exactly the same as we defined them in our previous `speech_to_text_route`. In the response we will use json.dumps function as we did before and will pass in a dictionary as a parameter containing `\"watsonxResponseText\":watsonx_response_text` and `\"watsonxResponseSpeech\":watsonx_response_speech`.\n",
    "\n",
    "We then return the `response`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Testing your personal assistant\n",
    "The assistant is now complete and ready to use.\n",
    "\n",
    "Now that we've updated the code quite considerably, it is a good time to rebuild our docker image and test to see that its working as expected in this environment.\n",
    "\n",
    "Assuming the Text-to-Speech and Speech-to-Text models URLs are correctly set, you just need to rebuild the image for the application and rerun it so it has all the latest changes.\n",
    "\n",
    "This step assumes that you have no running container for the application. If you do, please press Crtl (^) and C at the same time to stop the container.\n",
    "\n",
    "```\n",
    "docker build . -t voice-translator-powered-by-watsonx\n",
    "docker run -p 8000:8000 voice-translator-powered-by-watsonx\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
