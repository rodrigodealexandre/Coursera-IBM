{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business AI Meeting Companion STT\n",
    "\n",
    "### Introduction\n",
    "Consider you're attending a business meeting where all conversations are being captured by an advanced AI application. This application not only transcribes the discussions with high accuracy but also provides a concise summary of the meeting, emphasizing the key points and decisions made.\n",
    "\n",
    "In our project, we'll use OpenAI's Whisper to transform speech into text. Next, we'll use IBM Watson's AI to summarize and find key points. We'll make an app with Hugging Face Gradio as the user interface.\n",
    "\n",
    "### Learning Objectives\n",
    "After finishing this lab, you will able to:\n",
    "\n",
    "* Create a Python script to generate text using a model from the Hugging Face Hub, identify some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.\n",
    "* Use OpenAI's Whisper technology to convert lecture recordings into text, accurately.\n",
    "* Implement IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.\n",
    "* Create an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators.\n",
    "\n",
    "### Preparing the environment\n",
    "Let's start with setting up the environment by creating a Python virtual environment and installing the required libraries, using the following commands in the terminal:\n",
    "\n",
    "```\n",
    "pip3 install virtualenv \n",
    "virtualenv my_env # create a virtual environment my_env\n",
    "source my_env/bin/activate # activate my_env\n",
    "```\n",
    "\n",
    "Then, install the required libraries in the environment (this will take time ☕️☕️):\n",
    "```\n",
    "# installing required libraries in my_env\n",
    "pip install transformers==4.35.2 torch==2.1.1 gradio==4.17.0 langchain==0.0.343 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.19.4\n",
    "```\n",
    "\n",
    "We need to install ffmpeg to be able to work with audio files in python:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "Then run: `sudo apt install ffmpeg -y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Speech-to-Text\n",
    "Initially, we want to create a simple speech-to-text Python file using OpenAI Whisper.\n",
    "\n",
    "You can test the sample audio file Sample voice link to [download](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3).\n",
    "\n",
    "Create and open a Python file and call it `simple_speech2text.py`.\n",
    "\n",
    "Let's download the file first (you can do it manually, then drag and drop it into the file environment).\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "# URL of the audio file to be downloaded\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3\"\n",
    "\n",
    "# Send a GET request to the URL to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Define the local file path where the audio file will be saved\n",
    "audio_file_path = \"downloaded_audio.mp3\"\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # If successful, write the content to the specified local file path\n",
    "    with open(audio_file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully\")\n",
    "else:\n",
    "    # If the request failed, print an error message\n",
    "    print(\"Failed to download the file\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement OpenAI Whisper for transcribing voice to speech.\n",
    "\n",
    "You can override the previous code in the Python file.\n",
    "\n",
    "```\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the speech-to-text pipeline from Hugging Face Transformers\n",
    "# This uses the \"openai/whisper-tiny.en\" model for automatic speech recognition (ASR)\n",
    "# The `chunk_length_s` parameter specifies the chunk length in seconds for processing\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=\"openai/whisper-tiny.en\",\n",
    "  chunk_length_s=30,\n",
    ")\n",
    "\n",
    "# Define the path to the audio file that needs to be transcribed\n",
    "sample = 'downloaded_audio.mp3'\n",
    "\n",
    "# Perform speech recognition on the audio file\n",
    "# The `batch_size=8` parameter indicates how many chunks are processed at a time\n",
    "# The result is stored in `prediction` with the key \"text\" containing the transcribed text\n",
    "prediction = pipe(sample, batch_size=8)[\"text\"]\n",
    "\n",
    "# Print the transcribed text to the console\n",
    "print(prediction)\n",
    "```\n",
    "\n",
    "Run the Python file and you will get the output.\n",
    "\n",
    "`python3 simple_speech2text.py`\n",
    "\n",
    "In the next step, we will utilize `Gradio` for creating interface for our app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio interface\n",
    "### Creating a simple demo\n",
    "Through this project, we will create different LLM applications with Gradio interface. Let's get familiar with Gradio by creating a simple app:\n",
    "\n",
    "Still in the `project` directory, create a Python file and name it hello.py`.\n",
    "\n",
    "Open `hello.py`, paste the following Python code and save the file.\n",
    "\n",
    "```\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)\n",
    "```\n",
    "\n",
    "The above code creates a **gradio.Interface** called `demo`. It wraps the `greet` function with a simple text-to-text user interface that you could interact with.\n",
    "\n",
    "The **gradio.Interface** class is initialized with 3 required parameters:\n",
    "\n",
    "* fn: the function to wrap a UI around\n",
    "* inputs: which component(s) to use for the input (e.g. “text”, “image” or “audio”)\n",
    "* outputs: which component(s) to use for the output (e.g. “text”, “image” or “label”)\n",
    "\n",
    "The last line `demo.launch()` launches a server to serve our `demo`.\n",
    "\n",
    "### Launching the demo app\n",
    "Now go back to the terminal and make sure that the `my_env` virtual environment name is displayed at the beginning of the line. Run the following command to execute the Python script.\n",
    "\n",
    "`python3 hello.py`\n",
    "\n",
    "As the Python code is served by a local host, click on the button below and you will be able to see the simple application we just created. Feel free to play around with the input and output of the web app!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Creating audio transcription app\n",
    "Create a new python file `speech2text_app.py`\n",
    "\n",
    "**Exercise: Complete the `transcript_audio` function.**\n",
    "\n",
    "From the step1: fill the missing parts in `transcript_audio` function.\n",
    "\n",
    "```\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# Function to transcribe audio using the OpenAI Whisper model\n",
    "def transcript_audio(audio_file):\n",
    "    # Initialize the speech recognition pipeline\n",
    "    pipe = #-----> Fill here <----\n",
    "    \n",
    "    # Transcribe the audio file and return the result\n",
    "    result = #-----> Fill here <----\n",
    "    return result\n",
    "\n",
    "# Set up Gradio interface\n",
    "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")  # Audio input\n",
    "output_text = gr.Textbox()  # Text output\n",
    "\n",
    "# Create the Gradio interface with the function, inputs, and outputs\n",
    "iface = gr.Interface(fn=transcript_audio, \n",
    "                     inputs=audio_input, outputs=output_text, \n",
    "                     title=\"Audio Transcription App\",\n",
    "                     description=\"Upload the audio file\")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "```\n",
    "\n",
    "Then, run your app:\n",
    "\n",
    "`python3 speech2text_app.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
