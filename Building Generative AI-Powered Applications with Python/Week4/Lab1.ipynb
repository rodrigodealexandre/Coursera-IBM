{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business AI Meeting Companion STT\n",
    "\n",
    "### Introduction\n",
    "Consider you're attending a business meeting where all conversations are being captured by an advanced AI application. This application not only transcribes the discussions with high accuracy but also provides a concise summary of the meeting, emphasizing the key points and decisions made.\n",
    "\n",
    "In our project, we'll use OpenAI's Whisper to transform speech into text. Next, we'll use IBM Watson's AI to summarize and find key points. We'll make an app with Hugging Face Gradio as the user interface.\n",
    "\n",
    "### Learning Objectives\n",
    "After finishing this lab, you will able to:\n",
    "\n",
    "* Create a Python script to generate text using a model from the Hugging Face Hub, identify some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.\n",
    "* Use OpenAI's Whisper technology to convert lecture recordings into text, accurately.\n",
    "* Implement IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.\n",
    "* Create an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators.\n",
    "\n",
    "### Preparing the environment\n",
    "Let's start with setting up the environment by creating a Python virtual environment and installing the required libraries, using the following commands in the terminal:\n",
    "\n",
    "```\n",
    "pip3 install virtualenv \n",
    "virtualenv my_env # create a virtual environment my_env\n",
    "source my_env/bin/activate # activate my_env\n",
    "```\n",
    "\n",
    "Then, install the required libraries in the environment (this will take time ☕️☕️):\n",
    "```\n",
    "# installing required libraries in my_env\n",
    "pip install transformers==4.35.2 torch==2.1.1 gradio==4.17.0 langchain==0.0.343 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.19.4\n",
    "```\n",
    "\n",
    "We need to install ffmpeg to be able to work with audio files in python:\n",
    "\n",
    "`sudo apt update`\n",
    "\n",
    "Then run: `sudo apt install ffmpeg -y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Speech-to-Text\n",
    "Initially, we want to create a simple speech-to-text Python file using OpenAI Whisper.\n",
    "\n",
    "You can test the sample audio file Sample voice link to [download](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3).\n",
    "\n",
    "Create and open a Python file and call it `simple_speech2text.py`.\n",
    "\n",
    "Let's download the file first (you can do it manually, then drag and drop it into the file environment).\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "# URL of the audio file to be downloaded\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3\"\n",
    "\n",
    "# Send a GET request to the URL to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Define the local file path where the audio file will be saved\n",
    "audio_file_path = \"downloaded_audio.mp3\"\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # If successful, write the content to the specified local file path\n",
    "    with open(audio_file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"File downloaded successfully\")\n",
    "else:\n",
    "    # If the request failed, print an error message\n",
    "    print(\"Failed to download the file\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement OpenAI Whisper for transcribing voice to speech.\n",
    "\n",
    "You can override the previous code in the Python file.\n",
    "\n",
    "```\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the speech-to-text pipeline from Hugging Face Transformers\n",
    "# This uses the \"openai/whisper-tiny.en\" model for automatic speech recognition (ASR)\n",
    "# The `chunk_length_s` parameter specifies the chunk length in seconds for processing\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=\"openai/whisper-tiny.en\",\n",
    "  chunk_length_s=30,\n",
    ")\n",
    "\n",
    "# Define the path to the audio file that needs to be transcribed\n",
    "sample = 'downloaded_audio.mp3'\n",
    "\n",
    "# Perform speech recognition on the audio file\n",
    "# The `batch_size=8` parameter indicates how many chunks are processed at a time\n",
    "# The result is stored in `prediction` with the key \"text\" containing the transcribed text\n",
    "prediction = pipe(sample, batch_size=8)[\"text\"]\n",
    "\n",
    "# Print the transcribed text to the console\n",
    "print(prediction)\n",
    "```\n",
    "\n",
    "Run the Python file and you will get the output.\n",
    "\n",
    "`python3 simple_speech2text.py`\n",
    "\n",
    "In the next step, we will utilize `Gradio` for creating interface for our app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio interface\n",
    "### Creating a simple demo\n",
    "Through this project, we will create different LLM applications with Gradio interface. Let's get familiar with Gradio by creating a simple app:\n",
    "\n",
    "Still in the `project` directory, create a Python file and name it hello.py`.\n",
    "\n",
    "Open `hello.py`, paste the following Python code and save the file.\n",
    "\n",
    "```\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)\n",
    "```\n",
    "\n",
    "The above code creates a **gradio.Interface** called `demo`. It wraps the `greet` function with a simple text-to-text user interface that you could interact with.\n",
    "\n",
    "The **gradio.Interface** class is initialized with 3 required parameters:\n",
    "\n",
    "* fn: the function to wrap a UI around\n",
    "* inputs: which component(s) to use for the input (e.g. “text”, “image” or “audio”)\n",
    "* outputs: which component(s) to use for the output (e.g. “text”, “image” or “label”)\n",
    "\n",
    "The last line `demo.launch()` launches a server to serve our `demo`.\n",
    "\n",
    "### Launching the demo app\n",
    "Now go back to the terminal and make sure that the `my_env` virtual environment name is displayed at the beginning of the line. Run the following command to execute the Python script.\n",
    "\n",
    "`python3 hello.py`\n",
    "\n",
    "As the Python code is served by a local host, click on the button below and you will be able to see the simple application we just created. Feel free to play around with the input and output of the web app!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Creating audio transcription app\n",
    "Create a new python file `speech2text_app.py`\n",
    "\n",
    "**Exercise: Complete the `transcript_audio` function.**\n",
    "\n",
    "From the step1: fill the missing parts in `transcript_audio` function.\n",
    "\n",
    "```\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# Function to transcribe audio using the OpenAI Whisper model\n",
    "def transcript_audio(audio_file):\n",
    "    # Initialize the speech recognition pipeline\n",
    "    pipe = #-----> Fill here <----\n",
    "    \n",
    "    # Transcribe the audio file and return the result\n",
    "    result = #-----> Fill here <----\n",
    "    return result\n",
    "\n",
    "# Set up Gradio interface\n",
    "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")  # Audio input\n",
    "output_text = gr.Textbox()  # Text output\n",
    "\n",
    "# Create the Gradio interface with the function, inputs, and outputs\n",
    "iface = gr.Interface(fn=transcript_audio, \n",
    "                     inputs=audio_input, outputs=output_text, \n",
    "                     title=\"Audio Transcription App\",\n",
    "                     description=\"Upload the audio file\")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "```\n",
    "\n",
    "Then, run your app:\n",
    "\n",
    "`python3 speech2text_app.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Integrating LLM: Using Llama 2 in WatsonX as LLM\n",
    "##### Running simple LLM\n",
    "Let's start by generating text with LLMs. Create a Python file and name it `simple_llm.py`. You can proceed by clicking the link below or by referencing the accompanying image.\n",
    "\n",
    "In case, you want to use Llama 2 as an LLM instance, you can follow the instructions below:\n",
    "\n",
    "*IBM WatsonX utilizes various language models, including Llama 2 by Meta, which is currently the strongest open-source language model.*\n",
    "\n",
    "Here's how the code works:\n",
    "\n",
    "1. **Setting up credentials**: The credentials needed to access IBM's services are pre-arranged by the Skills Network team, so you don't have to worry about setting them up yourself.\n",
    "\n",
    "2. **Specifying parameters**: The code then defines specific parameters for the language model. 'MAX_NEW_TOKENS' sets the limit on the number of words the model can generate in one go. 'TEMPERATURE' adjusts how creative or predictable the generated text is.\n",
    "\n",
    "3. **Setting up Llama 2 model**: Next, the LLAMA2 model is set up using a model ID, the provided credentials, chosen parameters, and a project ID.\n",
    "\n",
    "4. **Creating an object for Llama 2**: The code creates an object named 'llm', which is used to interact with the LLAMA2 model. A model object, `LLAMA2_model`, is created using the Model class, which is initialized with a specific model ID, credentials, parameters, and project ID. Then, an instance of WatsonxLLM is created with `LLAMA2_model` as an argument, initializing the language model hub 'llm' object.\n",
    "\n",
    "5. **Generating and printing response**: Finally, 'llm' is used to generate a response to the question, \"How to read a book effectively?\" The response is then printed out.\n",
    "\n",
    "```\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "my_credentials = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.\n",
    "        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
    "    }\n",
    "\n",
    "LLAMA2_model = Model(\n",
    "        model_id= 'meta-llama/llama-2-70b-chat', \n",
    "        credentials=my_credentials,\n",
    "        params=params,\n",
    "        project_id=\"skills-network\",  \n",
    "        )\n",
    "\n",
    "llm = WatsonxLLM(LLAMA2_model)  \n",
    "\n",
    "print(llm(\"How to read a book effectively?\"))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Put them all together\n",
    "Create a new Python file and call it `speech_analyzer.py`\n",
    "\n",
    "In this exercise, we'll set up a language model (LLM) instance, which could be IBM WatsonxLLM, HuggingFaceHub, or an OpenAI model. Then, we'll establish a prompt template. These templates are structured guides to generate prompts for language models, aiding in output organization (more info in langchain prompt template.\n",
    "\n",
    "Next, we'll develop a transcription function that employs the OpenAI Whisper model to convert speech-to-text. This function takes an audio file uploaded through a Gradio app interface (preferably in .mp3 format). The transcribed text is then fed into an LLMChain, which integrates the text with the prompt template and forwards it to the chosen LLM. The final output from the LLM is then displayed in the Gradio app's output textbox.\n",
    "\n",
    "The output should look:\n",
    "\n",
    "![scr](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/images/final_result_whisper.jpg)\n",
    "\n",
    "Notice how the LLM corrected a minor mistake made by the speech-to-text model, resulting in a coherent and accurate output.\n",
    "\n",
    "##### Exercise: Fill the missing parts:\n",
    "\n",
    "```\n",
    "import torch\n",
    "import os\n",
    "import gradio as gr\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "\n",
    "#######------------- LLM-------------####\n",
    "\n",
    "# initiate LLM instance, this can be IBM WatsonX, huggingface, or OpenAI instance\n",
    "\n",
    "llm = ###---> write your code here\n",
    "\n",
    "#######------------- Prompt Template-------------####\n",
    "\n",
    "# This template is structured based on LLAMA2. If you are using other LLMs, feel free to remove the tags\n",
    "temp = \"\"\"\n",
    "<s><<SYS>>\n",
    "List the key points with details from the context: \n",
    "[INST] The context : {context} [/INST] \n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "# here is the simplified version of the prompt template\n",
    "# temp = \"\"\"\n",
    "# List the key points with details from the context: \n",
    "# The context : {context} \n",
    "# \"\"\"\n",
    "\n",
    "pt = PromptTemplate(\n",
    "    input_variables=[\"context\"],\n",
    "    template= temp)\n",
    "\n",
    "prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)\n",
    "\n",
    "#######------------- Speech2text-------------####\n",
    "\n",
    "def transcript_audio(audio_file):\n",
    "    # Initialize the speech recognition pipeline\n",
    "    \n",
    "    pipe = #------> write the code here\n",
    "    \n",
    "    # Transcribe the audio file and return the result\n",
    "    transcript_txt = pipe(audio_file, batch_size=8)[\"text\"]\n",
    "    # run the chain to merge transcript text with the template and send it to the LLM\n",
    "    result = prompt_to_LLAMA2.run(transcript_txt) \n",
    "\n",
    "    return result\n",
    "\n",
    "#######------------- Gradio-------------####\n",
    "\n",
    "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")\n",
    "output_text = gr.Textbox()\n",
    "\n",
    "# Create the Gradio interface with the function, inputs, and outputs\n",
    "iface = #---> write code here\n",
    "\n",
    "iface.launch(server_name=\"0.0.0.0\", server_port=7860)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
