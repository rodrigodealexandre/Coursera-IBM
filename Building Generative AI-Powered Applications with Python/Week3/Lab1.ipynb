{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Voice Assistant with OpenAI's GPT-3 and IBM Watson\n",
    "\n",
    "Introduction\n",
    "Welcome to this guided project on creating a voice assistant using OpenAI and IBM Watson Speech Libraries for Embed. The guided project takes you through building a virtual assistant that can take voice input, convert it to text using speech-to-text technology, send the text to OpenAI's GPT-3 model, receive a response, convert it to speech using text-to-speech technology and finally play it back to the user. The voice assistant will have a responsive front-end using HTML, CSS, and JavaScript, and a reliable back-end using Flask.\n",
    "\n",
    "Click here to play with a demo of the final application that you will create!\n",
    "\n",
    "By the end of the course, you will have a deep understanding of voice assistants and the skills to create your own AI-powered assistant that can communicate through voice input and output. You will also have a strong foundation in web development using Python, Flask, HTML, CSS, and JavaScript, and a finished full stack impressive application!\n",
    "\n",
    "Before you begin, let's give some context of each topic.\n",
    "\n",
    "OpenAI\n",
    "OpenAI is a research organization that aims to promote and develop friendly artificial intelligence in a way that benefits humanity as a whole. One of their key projects is GPT-3, which is a state-of-the-art natural language processing model. You will be using GPT-3 in your assistant to allow it to understand and respond to a wide range of user inputs.\n",
    "\n",
    "IBM Watson speech libraries for embed\n",
    "IBM Watson® Speech Libraries for Embed are a set of containerized text-to-speech and speech-to-text libraries designed to offer our IBM partners greater flexibility to infuse the best of IBM Research® technology into their solutions. Now available as embeddable AI, partners gain greater capabilities to build voice transcription and voice synthesis applications more quickly and deploy them in any hybrid multi-cloud environment. These technologies allow the assistant to communicate with users through voice input and output.\n",
    "\n",
    "Voice assistants\n",
    "A virtual assistant is a program designed to simulate conversation with human users, especially over the Internet using natural human voice. Assistants can be used in a variety of industries, including customer service, e-commerce, and education.\n",
    "\n",
    "Python (Flask)\n",
    "Python is a popular programming language that is widely used in web development and data science. Flask is a web framework for Python that makes it easy to build web applications. You will be using Python and Flask to build the backend of your voice assistant. Python is a powerful language that is easy to learn and has a large ecosystem of libraries and frameworks that can be leveraged in projects like yours.\n",
    "\n",
    "HTML - CSS - Javascript\n",
    "HTML (Hypertext Markup Language) is a markup language used to structure content on the web. CSS (Cascading Style Sheets) is a stylesheet language used to describe the look and formatting of a document written in HTML. Javascript is a programming language that is commonly used to add interactivity to web pages. Together, these technologies allow us to build a visually appealing and interactive frontend for your assistant. Users will be able to interact with the voice assistant through a web interface that's built using HTML, CSS, and Javascript.\n",
    "\n",
    "Learning objectives\n",
    "At the end of this project, you will be able to:\n",
    "\n",
    "Explain the basics of voice assistants and their various applications\n",
    "Set up a development environment for building an assistant using Python, Flask, HTML, CSS, and Javascript\n",
    "Implement speech-to-text functionality to allow the assistant to understand voice input from users\n",
    "Integrate the assistant with OpenAI's GPT-3 model to give it a high level of intelligence and the ability to understand and respond to user requests\n",
    "Implement text-to-speech functionality to allow the assistant to communicate with users through voice output\n",
    "Combine all the above components to create a functional assistant that can take voice input and provide a spoken response\n",
    "(Optional) Deploy the assistant to a web server for use by a wider audience\n",
    "Prerequisites\n",
    "Having knowledge of the basics of HTML/CSS, Javascript, and Python are nice to have but not essential. We will do our best in explaining each step of the process as well as any code shown along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the interface\n",
    "In this project, the goal is to create an interface that allows communication with a voice assistant, and a backend to manage the sending and receiving of responses.\n",
    "\n",
    "The frontend will use HTML, CSS and Javascript with popular libraries such as Bootstrap for basic styling, Font Awesome for icons and JQuery for efficient handling of actions. The user interface will be similar to other voice assistant applications, like Google Assistant. The code for the interface is provided and the focus of the course is on building the voice assistant and integrating it with various services and APIs. The provided code will help you to understand how the frontend and backend interact, and as you go through it, you will learn about the important parts and how it works, giving you a good understanding of how the frontend works and how to create this simple web page.\n",
    "\n",
    "Run the following commands to receive the outline of the project, rename it to save it with another name and finally move into that directory.\n",
    "\n",
    "```\n",
    "git clone https://github.com/arora-r/chatapp-with-voice-and-openai-outline.git\n",
    "mv chatapp-with-voice-and-openai-outline chatapp-with-voice-and-openai\n",
    "cd chatapp-with-voice-and-openai\n",
    "```\n",
    "\n",
    "The next section gives a brief understanding of how the frontend works.\n",
    "\n",
    "#### HTML, CSS, and Javascript\n",
    "The `index.html` file is responsible for the layout and structure of the web interface. This file contains the code for incorporating external libraries such as JQuery, Bootstrap, and FontAwesome Icons, as well as the CSS (`style.css`) and Javascript code (`script.js`) that control the styling and interactivity of the interface.\n",
    "\n",
    "The `style.css` file is responsible for customizing the visual appearance of the page's components. It also handles the loading animation using CSS keyframes. Keyframes are a way of defining the values of an animation at various points in time, allowing for a smooth transition between different styles and creating dynamic animations.\n",
    "\n",
    "The `script.js` file is responsible for the page's interactivity and functionality. It contains the majority of the code and handles all the necessary functions such as switching between light and dark mode, sending messages, and displaying new messages on the screen. It even enables the users to record audio.\n",
    "\n",
    "#### Images of UI\n",
    "Here are some images of the frontend you received.\n",
    "\n",
    "##### Light mode\n",
    "This demonstrates how the base code works. It'll just return null as a response\n",
    "\n",
    "##### Dark mode\n",
    "Once you go through the project, you'll complete the assistant and it will be able to give clear responses as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Understanding the server\n",
    "The server is how the application will run and communicate with all your services. Flask is a web development framework for Python and can be used as a backend for the application. It is a lightweight and simple framework that makes it quick and easy to build web applications.\n",
    "\n",
    "With Flask, you can create web pages and applications without needing to know a lot of complex coding or use additional tools or libraries. You can create your own routes and handle user requests, and it also allows you to connect to external APIs and services to retrieve or send data.\n",
    "\n",
    "This guided project uses Flask to handle the backend of your voice assistant. This means that you will be using Flask to create routes and handle HTTP requests and responses. When a user interacts with the voice assistant through the frontend interface, the request will be sent to the Flask backend. Flask will then process the request and send it to the appropriate service.\n",
    "\n",
    "The code provided gives the outline for the server in the `server.py` file.\n",
    "\n",
    "At the top of the file, there are several import statements. These statements are used to bring in external libraries and modules, which will be used in the current file. For instance, `speech_text` is a function inside the `worker.py` file, while `openai` is a package that needs to be installed to use the OpenAI's GPT-3 model. These imported packages, modules and libraries will allow you to access the additional functionalities and methods that they offer, making it easy to interact with the speech-to-text and GPT-3 model in your code.\n",
    "\n",
    "Underneath the imports, the Flask application is initialized, and a CORS policy is set. A CORS policy is used to allow or prevent web pages from making requests to different domains than the one that served the web page. Currently, it is set to `*` to allow any request.\n",
    "\n",
    "The `server.py` file consists of 3 functions which are defined as routes, and the code to start the server.\n",
    "\n",
    "The first route is:\n",
    "\n",
    "```\n",
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "```\n",
    "\n",
    "When a user tries to load the application, they initially send a request to go to the `/` endpoint. They will then trigger this `index` function above and execute the code above. Currently, the returned code from the function is a render function to show the `index.html` file which is the frontend interface.\n",
    "\n",
    "The second and third routes are what will be used to process all requests and handle sending information between the applications.\n",
    "\n",
    "Finally, the application is started with the `app.run` command to run on port `8080` and have the host be `0.0.0.0` (a.k.a. `localhost`).\n",
    "\n",
    "The next sections will take you through the process of completing the `process_message_route` and `speech_to_text_route` functions in this file and help you understand how to use the packages and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Running the application\n",
    "Docker allows for the creation of “containers” that package an application and its dependencies together. This allows the application to run consistently across different environments, as the container includes everything it needs to run. Additionally, using a Docker image to create and run applications can simplify the deployment process, as the image can be easily distributed and run on any machine that has Docker installed. This can help to ensure that the application runs in the same way in development, testing, and production environments.\n",
    "\n",
    "The `git clone` from Step 1 already comes with a `Dockerfile` and `requirements.txt` for this application. These files are used to build the image with the dependencies already installed. Looking into the `Dockerfile` you can see its fairly simple, it just creates a python environment, moves all the files from the local directory to the container, installs the required packages, and then starts the application by running the `python` command.\n",
    "\n",
    "3 different containers need to run simultaneously to have the application run and interact with Text-to-Speech and Speech-to-Text capabilities.\n",
    "\n",
    "##### Small prerequisites:\n",
    "You need to run these commands with a single click to fulfill some of the prerequisites:\n",
    "\n",
    "```\n",
    "mkdir /home/project/chatapp-with-voice-and-openai/certs/\n",
    "cp /usr/local/share/ca-certificates/rootCA.crt /home/project/chatapp-with-voice-and-openai/certs/\n",
    "```\n",
    "\n",
    "#### 1. Starting the application\n",
    "This image is quick to build as the application is quite small. These commands first build the application (running the commands in the `Dockerfile`) and tags (names) the built container as `voice-chatapp-powered-by-openai`, then runs it in the foreground on `port 8000`. **You'll need to run these commands everytime you wish to make a new change to one of the files**.\n",
    "\n",
    "```\n",
    "docker build . -t voice-chatapp-powered-by-openai\n",
    "docker run -p 8000:8000 voice-chatapp-powered-by-openai\n",
    "```\n",
    "\n",
    "The application must be opened on a new tab since the minibrowser in this environment cannot support certain required features.\n",
    "\n",
    "Your browser may deny “pop-ups” but please allow them for the new tab to open up.\n",
    "\n",
    "At this point, the application will run but return `null` for any input.\n",
    "\n",
    "Once you've had a chance to run and play around with the application, please press `Crtl` (a.k.a. `control (^)` for Mac) and `C` at the same time to stop the container and continue the project.\n",
    "\n",
    "The application will only run while the container is up. If you make new changes to the files and would like to test them, you will have to rebuild the image.\n",
    "\n",
    "#### 2. Starting Speech-to-Text\n",
    "Skills Network provides its own Watson Speech-to-Text image that runs automatically in this environment. To access it, use this endpoint URL when you get to Step 4:\n",
    "\n",
    "`base_url = \"https://sn-watson-stt.labs.skills.network\"`\n",
    "\n",
    "You can test it works by running this query:\n",
    "\n",
    "`curl https://sn-watson-stt.labs.skills.network/speech-to-text/api/v1/models`\n",
    "\n",
    "**(it does not work local)**\n",
    "\n",
    "#### 3. Starting Text-to-Speech\n",
    "Skills Network provides its own Watson Text-to-Speech image that is run automatically in this environment. To access it, use this endpoint URL when you get to Step 6:\n",
    "\n",
    "`base_url = \"https://sn-watson-tts.labs.skills.network\"`\n",
    "\n",
    "You can test it works by running this query:\n",
    "\n",
    "`curl https://sn-watson-tts.labs.skills.network/text-to-speech/api/v1/voices`\n",
    "\n",
    "**(it does not work local)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Integrating Watson Speech-to-Text\n",
    "Speech-to-Text functionality is a technology that converts speech into text using machine learning. It is useful for accessibility, productivity, convenience, multilingual support, and cost-effective solutions for a wide range of applications. For example, being able to take a user's voice as input for a chat application.\n",
    "\n",
    "Using the embedded Watson Speech-to-Text AI model that was deployed earlier, it is possible to easily convert your speech-to-text by a simple API. This result can then be passed to OpenAI API for generating a response.\n",
    "\n",
    "#### Implementation\n",
    "You will be updating a function called `speech_to_text` that will take in audio data received from the browser and pass it to the Watson Speech-to-Text API. Open `worker.py` from the explore or by clicking below.\n",
    "\n",
    "It's important to import the requests library at the top of your `worker.py` file. This library is a simple HTTP request library that you will be using to make API calls to the Watson Speech-to-Text API.\n",
    "\n",
    "The speech_to_text function will take in audio data as a parameter, make an API call to the Watson Speech-to-Text API using the requests library, and return the transcription of the audio data.\n",
    "\n",
    "**Remember to replace the `...` for the `base_url` variable with the URL for your Speech-to-Text model (for example, `https://sn-watson-stt.labs.skills.network`).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
