{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Meaningful Names to Your Photos with AI\n",
    "\n",
    "### Introduction\n",
    "Images, rich with untapped information, often come under the radar of search engines and data systems. Transforming this visual data into machine-readable language is no easy task, but it's where image captioning AI is useful. Here's how image captioning AI can make a difference:\n",
    "\n",
    "* Improves accessibility: Helps visually impaired individuals understand visual content.\n",
    "* Enhances SEO: Assists search engines in identifying the content of images.\n",
    "* Facilitates content discovery: Enables efficient analysis and categorization of large image databases.\n",
    "* Supports social media and advertising: Automates engaging description generation for visual content.\n",
    "* Boosts security: Provides real-time descriptions of activities in video footage.\n",
    "* Aids in education and research: Assists in understanding and interpreting visual materials.\n",
    "* Offers multilingual support: Generates image captions in various languages for international audiences.\n",
    "* Enables data organization: Helps manage and categorize large sets of visual data.\n",
    "* Saves time: Automated captioning is more efficient than manual efforts.\n",
    "* Increases user engagement: Detailed captions can make visual content more engaging and informative.\n",
    "\n",
    "### Learning objectives\n",
    "At the end of this project, you will be able to:\n",
    "\n",
    "* Implement an image captioning tool using the BLIP model from Hugging Face's Transformers\n",
    "* Use Gradio to provide a user-friendly interface for your image captioning application\n",
    "* Adapt the tool for real-world business scenarios, demonstrating its practical \n",
    "\n",
    "![Generate by AI and enhanced by human](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX057EN/images/image_captioning_1.png)\\\n",
    "Generate by AI and enhanced by human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment and installing libraries\n",
    "In this project, to build an AI app, you will use Gradio interface provided by Hugging Face.\n",
    "\n",
    "Let's set up the environment and dependencies for this project. Open up a new terminal and make sure you are in the `home/project` directory.\n",
    "\n",
    "Open a new terminal\n",
    "\n",
    "Create a Python virtual environment and install Gradio using the following commands in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip3 install virtualenv \n",
    "virtualenv my_env # create a virtual environment my_env\n",
    "source my_env/bin/activate # activate my_env\n",
    "\n",
    "# installing required libraries in my_env\n",
    "pip install langchain==0.1.11 gradio==4.21.0 transformers==4.38.2 bs4==0.0.2 requests==2.31.0 torch==2.2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a cup of coffee, it will take 5 minutes.\n",
    "\n",
    "```\n",
    "      )  (\n",
    "     (   ) )\n",
    "      ) ( (\n",
    "    _______)_\n",
    " .-'---------|  \n",
    "( C|/\\/\\/\\/\\/|\n",
    " '-./\\/\\/\\/\\/|\n",
    "   '_________'\n",
    "    '-------'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating image captions with the BLIP model\n",
    "#### Introducing: Hugging Face, Tranformers, and BLIP\n",
    "Hugging Face is an organization that focuses on natural language processing (NLP) and artificial intelligence (AI).The organization is widely known for its open-source library called \"Transformers\" which provides thousands of pre-trained models to the community. The library supports a wide range of NLP tasks, such as translation, summarization, text generation, and more. Transformers has contributed significantly to the recent advancements in NLP, as it has made state-of-the-art models, such as BERT, GPT-2, and GPT-3, accessible to researchers and developers worldwide.\n",
    "\n",
    "Tranformers library includes a model that can be used to capture information from images. The BLIP, or Bootstrapping Language-Image Pre-training, model is a tool that helps computers understand and generate language based on images. It's like teaching a computer to look at a picture and describe it, or answer questions about it.\n",
    "\n",
    "Alright, now that you know what BLIP can do, let's get started with implementing a simple image captioning AI app!\n",
    "\n",
    "#### Step 1: Import your required tools from the transformers library\n",
    "You have already installed the package `transformers` during setting up the environment.\n",
    "\n",
    "In the `project` directory, create a Python file and name it `image_cap.py`, copy the various code segments below and paste them into the Python file.\n",
    "\n",
    "You will be using `AutoProcessor` and `BlipForConditionalGeneration` from the `transformers` library.\n",
    "\n",
    "\"Blip2Processor\" and \"Blip2ForConditionalGeneration\" are components of the BLIP model, which is a vision-language model available in the Hugging Face Transformers library.\n",
    "\n",
    "* **AutoProcessor** : This is a processor class that is used for preprocessing data for the BLIP model. It wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor. This means it can handle both image and text data, preparing it for input into the BLIP model.\n",
    "\n",
    "    *Note*: *A tokenizer is a tool in natural language processing that breaks down text into smaller, manageable units (tokens), such as words or phrases, enabling models to analyze and understand the text.*\n",
    "\n",
    "* **BlipForConditionalGeneration** : This is a model class that is used for conditional text generation given an image and an optional text prompt. In other words, it can generate text based on an input image and an optional piece of text. This makes it useful for tasks like image captioning or visual question answering, where the model needs to generate text that describes an image or answer a question about an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# Load the pretrained processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Fetch the model and initialize a tokenizer\n",
    "After loading the processor and the model, you need to initialize the image to be captioned. The image data needs to be loaded and pre-processed to be ready for the model.\n",
    "\n",
    "To load the image right-click anywhere in the Explorer (on the left side of code pane), and click `Upload Files...` (shown in image below). You can upload any image from your local files, and modify the `img_path` according to the name of the image.\n",
    "\n",
    "In the next phase, you fetch an image, which will be captioned by your pre-trained model. This image can either be a local file or fetched from a URL. The Python Imaging Library, PIL, is used to open the image file and convert it into an RGB format which is suitable for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image, DONT FORGET TO WRITE YOUR IMAGE NAME\n",
    "img_path = \"YOUR IMAGE NAME.jpeg\"\n",
    "# convert it into an RGB format \n",
    "image = Image.open(img_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the pre-processed image is passed through the processor to generate inputs in the required format. The return_tensors argument is set to \"pt\" to return PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need a question for image captioning\n",
    "text = \"the image of\"\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You then pass these inputs into your model's generate method. The argument `max_new_tokens=50` specifies that the model should generate a caption of up to 50 tokens in length.\n",
    "\n",
    "* The two asterisks (**) in Python are used in function calls to unpack dictionaries and pass items in the dictionary as keyword arguments to the function. `**inputs` is unpacking the inputs dictionary and passing its items as arguments to the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a caption for the image\n",
    "outputs = model.generate(**inputs, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the generated output is a sequence of tokens. To transform these tokens into human-readable text, you use the decode method provided by the processor. The `skip_special_tokens` argument is set to `True` to ignore special tokens in the output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated tokens to text\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "# Print the caption\n",
    "print(caption)\n",
    "\n",
    "% the image of a man in a suit and tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you have the image's caption, generated by your model! This caption is a textual representation of the content of the image, as interpreted by the BLIP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image captioning app with Gradio\n",
    "Now that you understand the mechanism of image captioning, let's create a proper application with an intuitive interface. You can utilize Gradio, a tool provided by Hugging Face, for this purpose. To begin, you will have a brief introduction to Gradio. Following that, as an exercise, you will be tasked with implementing the image captioning application using the Gradio interface.\n",
    "\n",
    "#### Quickstart Gradio: Creating a simple demo\n",
    "Let's get familiar with Gradio by creating a simple app:\n",
    "\n",
    "Still in the project directory, create a Python file and name it `hello.py`.\n",
    "\n",
    "Open `hello.py`, copy and paste the following Python code and save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a **gradio.Interface** called `demo`. It wraps the `greet` function with a simple text-to-text user interface that you could interact with.\n",
    "\n",
    "The **gradio.Interface** class is initialized with 3 required parameters:\n",
    "\n",
    "* fn: the function to wrap a UI around\n",
    "* inputs: which component(s) to use for the input (e.g. “text”, “image” or “audio”)\n",
    "* outputs: which component(s) to use for the output (e.g. “text”, “image” or “label”)\n",
    "\n",
    "The last line `demo.launch()` launches a server to serve your `demo`.\n",
    "\n",
    "#### Launching the demo app\n",
    "Now go back to the terminal and make sure that the `my_env` virtual environment name is displayed at the begining of the line. Now run the following command to execute the Python script.\n",
    "\n",
    "`python3 hello.py`\n",
    "\n",
    "As the Python code is served by local host, click the button below and you will be able to see the simple application you created. Feel free to play around with the input and output of the web app!\n",
    "\n",
    "You should see the following, here the name entered is bob:\n",
    "![Input and output](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0ZLFEN/images/bob.png)\n",
    "\n",
    "If you finish playing with the app and want to exit, press `ctrl+c` in the terminal and close the application tab.\n",
    "\n",
    "You just had a first taste of the Gradio interface, it's easy right? If you wish to learn a little bit more about customization in Gradio, you are invited to take the guided project called Bring your Machine Learning model to life with Gradio. You can find it under Courses & Projects on cognitiveclass.ai!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Implement image captioning app with Gradio\n",
    "In this exercise, you will walk through the steps to create a web application that generates captions for images using the BLIP-2 model and the Gradio library. Follow the steps below:\n",
    "\n",
    "#### Step 1: Set up the environment\n",
    "\n",
    "* Make sure you have the necessary libraries installed. Run `pip install gradio transformers Pillow` to install Gradio, Transformers, and Pillow.\n",
    "* Import the required libraries:\n",
    "\n",
    "Now, let's create a new Python file and call it `image_captioning_app.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load the pretrained model\n",
    "\n",
    "Load the pretrained processor and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = # write your code here\n",
    "model = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define the image captioning function\n",
    "\n",
    "* Define the `caption_image` function that takes an input image and returns a caption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_image(input_image: np.ndarray):\n",
    "    # Convert numpy array to PIL Image and convert to RGB\n",
    "    raw_image = Image.fromarray(input_image).convert('RGB')\n",
    "    \n",
    "    # Process the image\n",
    "    \n",
    "\n",
    "    # Generate a caption for the image\n",
    "\n",
    "\n",
    "    # Decode the generated tokens to text and store it into `caption`\n",
    "    \n",
    "\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create the Gradio interface\n",
    "\n",
    "Use the `gr.Interface` class to create the web app interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn=caption_image, \n",
    "    inputs=gr.Image(), \n",
    "    outputs=\"text\",\n",
    "    title=\"Image Captioning\",\n",
    "    description=\"This is a simple web app for generating captions for images using a trained model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Launch the Web App\n",
    "\n",
    "Start the web app by calling the `launch()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Run the application\n",
    "\n",
    "Save the complete code to a Python file, for example, `image_captioning_app.py`.\n",
    "Open a terminal or command prompt, navigate to the directory where the file is located, and run the command\n",
    "\n",
    "`python3 image_captioning_app.py`\n",
    "\n",
    "Press `ctrl + c` to quit the application.\n",
    "\n",
    "You will have such output in the new windows:\n",
    "\n",
    "![img](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX057EN/images/output_image_captioning_app.png)\n",
    "\n",
    "**If you are running locally: Interact with the web App:**\n",
    "* The web app should start running and display a URL where you can access the interface.\n",
    "* Open the provided URL in a web browser (in the terminal).\n",
    "* You should see an interface with an image upload box.\n",
    "\n",
    "Congratulations! You have created an image captioning web app using Gradio and the BLIP model. You can further customize the interface, modify the code, or experiment with different models and settings to enhance the application's functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario: How image captioning helps a business\n",
    "\n",
    "##### Business scenario on news and media:\n",
    "A news agency publishes hundreds of articles daily on its website. Each article contains several images relevant to the story. Writing appropriate and descriptive captions for each image manually is a tedious task and might slow down the publication process.\n",
    "\n",
    "In this scenario, your image captioning program can expedite the process:\n",
    "\n",
    "1. Journalists write their articles and select relevant images to go along with the story.\n",
    "2. These images are then fed into the image captioning program (instead of manually insert description for each image).\n",
    "3. The program processes these images and generates a text file with the suggested captions for each image.\n",
    "4. The journalists or editors review these captions. They might use them as they are, or they might modify them to better fit the context of the article.\n",
    "5. These approved captions then serve a dual purpose:\n",
    "    * Enhanced accessibility: The captions are integrated as alternative text (alt text) for the images in the online article. Visually impaired users, using screen readers, can understand the context of the images through these descriptions. It helps them to have a similar content consumption experience as sighted users, adhering to the principles of inclusive and accessible design.\n",
    "    * Improved SEO: Properly captioned images with relevant keywords improve the article's SEO. Search engines like Google consider alt text while indexing, and this helps the article to appear in relevant search results, thereby driving organic traffic to the agency's website. This is especially useful for image search results.\n",
    "\n",
    "6. Once the captions are approved, they are added to the images in the online article.\n",
    "\n",
    "By integrating this process, the agency not only expedites its publication process but also ensures all images come with appropriate descriptions, enhancing the accessibility for visually impaired readers, and improving the website's SEO. This way, the agency broadens its reach and engagement with a more diverse audience base.\n",
    "\n",
    "##### Let's implement automated image captioning tool\n",
    "In this section, you implement an automated image captioning program that works directly from a URL. The user provides the URL, and the code generates captions for the images found on the webpage. The output is a text file that includes all the image URLs along with their respective captions (like the image below). To accomplish this, you use BeautifulSoup for parsing the HTML content of the page and extracting the image URLs.\n",
    "\n",
    "![Image urls](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX057EN/images/image_capturs_txt.png)\n",
    "\n",
    "Let's get started:\n",
    "\n",
    "Firstly, you send a HTTP request to the provided URL and retrieve the webpage's content. This content is then parsed by BeautifulSoup, which creates a parse tree from page's HTML. You look for 'img' tags in this parse tree as they contain the links to the images hosted on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/IBM\"\n",
    "\n",
    "# Download the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
