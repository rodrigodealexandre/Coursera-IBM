{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-based Web Application develpoment and deployment\n",
    "Estimated Time: 60 minutes\n",
    "\n",
    "#### Overview\n",
    "In this project, we make use of the embedded Watson AI libraries, to create an application that would perform sentiment analysis on a provided text. We then deploy the said application over the web using Flask framework.\n",
    "\n",
    "#### Project guidelines\n",
    "For the completion of this project, you'll have to complete the following 8 tasks, based on the knowledge you have gained through the course.\n",
    "\n",
    "#### Tasks and objectives:\n",
    "* Task 1: Clone the project repository\n",
    "* Task 2: Create a sentiment analysis application using Watson NLP library\n",
    "* Task 3: Format the output of the application\n",
    "* Task 4: Package the application\n",
    "* Task 5: Run Unit tests on your application\n",
    "* Task 6: Deploy as web application using Flask\n",
    "* Task 7: Incorporate Error handling\n",
    "* Task 8: Run static code analysis\n",
    "Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Embeddable Watson AI libraries\n",
    "In this project, you'll be using embeddable libraries to create an AI powered Python application.\n",
    "\n",
    "[Embeddable Watson AI libraries](http://https//www.ibm.com/docs/en/watson-libraries) include the NLP library, the text-to-speech library and the speech-to-text library. These libraries can be embedded and distributed as part of your application. For your convenience, these libraries have been pre-installed on Skills Network Labs Cloud IDE for use in this project.\n",
    "\n",
    "The NLP library includes functions for sentiment analysis, emotion detection, text classification, language detection, etc. among others. The speech-to-text library contains functions that perform the transcription service and generates written text from spoken audio. The text-to-speech library generates natural sounding audio from written text. All available functions, in each of these libraries, calls pretrained AI models that are all available on the Cloud IDE servers, available to all users for free.\n",
    "\n",
    "These libraries may also be accessed through your personal systems. The guidelines for the same are available on the Watson AI library page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Clone the project repository\n",
    "The Github repository of the project is available on the URL mentioned below.\n",
    "\n",
    "`https://github.com/ibm-developer-skills-network/zzrjt-practice-project-emb-ai.git`\n",
    "\n",
    "Clone this GitHub repo, using the Cloud IDE terminal to your project to a folder named `practice_project`. Once the cloning is complete, use the terminal to change the current directory `practice_project`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Create a sentiment analysis application using Watson NLP library\n",
    "NLP sentiment analysis is the practice of using computers to recognize sentiment or emotion expressed in a text. Through NLP, sentiment analysis categorizes words as positive, negative or neutral.\n",
    "\n",
    "Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understanding customer needs. It helps attain the attitude and mood of the wider public which can then help gather insightful information about the context.\n",
    "\n",
    "For creating the sentiment analysis application, we'll be making use of the Watson Embedded AI Libraries. Since the functions of these libraries are already deployed on the Cloud IDE server, there is no need of importing these libraries to our code. Instead, we need to send a POST request to the relevant model with the required text and the model will send the appropriate response.\n",
    "\n",
    "A sample code for such an application could be:\n",
    "```\n",
    "import requests\n",
    "\n",
    "def <function_name>(<input_args>):\n",
    "    url = '<relevant_url>'\n",
    "    headers = {<header_dictionary>}\n",
    "    myobj = {<input_dictionary_to_the_function>}\n",
    "    response = requests.post(url, json = myobj, headers=header)\n",
    "    return response.text\n",
    "```\n",
    "\n",
    "**`import requests` comes from `pip3 install requests` so you can run the import local, however it won't work since it does not have watson library installed and the link is only avaliable inside the lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Note: The response of the Watson NLP functions is in the form of an object. For accessing the details of the response, we can use `text` attribute of the object by calling `response.text` and make the function return the response as simple text.*\n",
    "\n",
    "For this project, you'll be using the BERT based Sentiment Analysis function of the Watson NLP Library. For accessing this funciton, the URL, the headers and the input json format is as follows.\n",
    "\n",
    "```\n",
    "URL: 'https://sn-watson-sentiment-bert.labs.skills.network/v1/watson.runtime.nlp.v1/NlpService/SentimentPredict'\n",
    "Headers: {\"grpc-metadata-mm-model-id\": \"sentiment_aggregated-bert-workflow_lang_multi_stock\"}\n",
    "Input json: { \"raw_document\": { \"text\": text_to_analyse } }\n",
    "```\n",
    "\n",
    "Here, `text_to_analyze` is being used as a variable that holds the actual written text which is to be analyzed.\n",
    "\n",
    "In this task, you need to create a file named `sentiment_analysis.py` in `practice_project` folder. In this file, write the function for running sentiment analysis using the Watson NLP BERT Seniment Analysis function, as discussed above. Let us call this function `setiment_analyzer`. Assume that that text to be analysed is passed to the function as an argument and is stored in the variable `text_to_analyse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Format the output of the application\n",
    "The output of the application created is in the form of a dictionary, but has been formatted as a text. To access relevant pieces of information from this output, we need to first convert this text into a dictionary. Since dictionaries are the default formatting system for JSON files, we make use of the in-built Python library `json`.\n",
    "\n",
    "Let's see how this works.\n",
    "\n",
    "First, in a Python shell, import the json library.\n",
    "`import json`\n",
    "\n",
    "Next, run the sentiment_analyzer function for the text “I love this new technology”, just like in Task 2, and store the output in a variable called `response`.\n",
    "\n",
    "```\n",
    "from sentiment_analysis import sentiment_analyzer\n",
    "response = sentiment_analyzer(\"I love this new technology\")\n",
    "```\n",
    "\n",
    "Now, pass the `response` variable as an argument to the json.loads function and save the output in `formatted_response`. Print `formatted_response` to see the difference in the formatting.\n",
    "\n",
    "```\n",
    "formatted_response = json.loads(response)\n",
    "print(formatted_response)\n",
    "```\n",
    "\n",
    "The expected output of the above mentioned steps is shown in the image below.\n",
    "![how it looks](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0224EN-Coursera/images/jsonify.png)\n",
    "\n",
    "Note that the absence of single quotes on either side on the response indicates that this is no longer a text, but is a dictionary instead. To access the correct information from this dictionary, we need to access the keys appropriately. Since this is a nested dictionary structure, i.e. a dictionary of dictionaries, the following statements need to be used to get the label and the score outputs from this response.\n",
    "\n",
    "```\n",
    "label = formatted_response['documentSentiment']['label']\n",
    "score = formatted_response['documentSentiment']['score']\n",
    "```\n",
    "\n",
    "Check the contents of `label` and `score` to verify the output.\n",
    "![how it looks](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0224EN-Coursera/images/jsonify_check.png)\n",
    "\n",
    "Now, for Task 3, incorporate the above mentioned technique and make changes to the `sentiment_analysis.py` file. The expected output from calling the `seniment_analyzer` function should now be a dictionary with 2 keys, label and score, each having the appropriate value extracted from the response of the Watson NLP function. Verify your changes by testing the modified function in a python shell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my code\n",
    "import requests, json\n",
    "\n",
    "def sentiment_analyzer(text_to_analyse):\n",
    "    url = 'https://sn-watson-sentiment-bert.labs.skills.network/v1/watson.runtime.nlp.v1/NlpService/SentimentPredict'\n",
    "    headers = {\"grpc-metadata-mm-model-id\": \"sentiment_aggregated-bert-workflow_lang_multi_stock\"}\n",
    "    myobj =  { \"raw_document\": { \"text\": text_to_analyse } }\n",
    "    response = requests.post(url, json = myobj, headers=headers)\n",
    "    \n",
    "    formatted_response = json.loads(response.text)\n",
    "    label = formatted_response['documentSentiment']['label']\n",
    "    score = formatted_response['documentSentiment']['score']\n",
    "    \n",
    "    return {\"label\": label, \"score\": score}\n",
    "\n",
    "\n",
    "# solution\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def sentiment_analyzer(text_to_analyse):\n",
    "   url = 'https://sn-watson-sentiment-bert.labs.skills.network/v1/watson.runtime.nlp.v1/NlpService/SentimentPredict'\n",
    "   myobj = { \"raw_document\": { \"text\": text_to_analyse } }\n",
    "   header = {\"grpc-metadata-mm-model-id\": \"sentiment_aggregated-bert-workflow_lang_multi_stock\"}\n",
    "   response = requests.post(url, json = myobj, headers=header)\n",
    "   formatted_response = json.loads(response.text)\n",
    "   label = formatted_response['documentSentiment']['label']\n",
    "   score = formatted_response['documentSentiment']['score']\n",
    "   return {'label': label, 'score': score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Package the application\n",
    "In this task, you have to package the final application you created in tasks 2 and 3.\n",
    "\n",
    "Let's keep the name of the package as `SentimentAnalysis`. The steps involved in packaging are:\n",
    "\n",
    "1. Create a folder in the working directory, with the name as the package name.\n",
    "2. Put (or move) the application code, also called module, in the package folder.\n",
    "3. Create the `__init__.py` file, referencing the module.\n",
    "\n",
    "`SentimentAnalysis` is now a valid package and can be imported into any file in this project.\n",
    "\n",
    "To test this, run a python shell in the terminal and try importing the `sentiment_analyzer` function from the package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Run Unit tests on your application\n",
    "Since now we have a functional aplication, it is required that we run unit tests on some test cases to check the validity of its outputs.\n",
    "\n",
    "For running unit tests, we need to create a new file that calls the required application function from the package and tests its for a known text and output pair.\n",
    "\n",
    "For this, complete the following steps.\n",
    "\n",
    "1. Create a new file in `practice_project` folder, called `test_sentiment_analysis.py`.\n",
    "2. In this file, import the `sentiment_analyzer` function from the `SentimentAnalysis` package. Also import the `unittest` library.\n",
    "3. Create the unit test class. Let's call it `TestSentimentAnalyzer`. Define test_sentiment_analyzer as the function to run the unit tests.\n",
    "Click here for the solution\n",
    "4. Define 3 unit tests in the said function and check for the validity of the following statement - label pairs.\\\n",
    "    * \"I love working with Python\": “SENT_POSITIVE\"\\\n",
    "    * \"I hate working with Pyhton\": “SENT_NEGATIVE\"\\\n",
    "    * \"I am neutral on Python\": \"SENT_NEUTRAL\"\n",
    "5. Call the unit tests.\\\n",
    "    `python3 test_sentiment_analysis.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my code\n",
    "from SentimentAnalysis.sentiment_analysis import sentiment_analyzer\n",
    "import unittest\n",
    "\n",
    "class TestSentimentAnalyzer(unittest.TestCase):\n",
    "    def test_sentiment_analyzer(self):\n",
    "        self.assertEqual(sentiment_analyzer(\"I love working with Python\")['label'],\"SENT_POSITIVE\")\n",
    "        self.assertEqual(sentiment_analyzer(\"I hate working with Pyhton\")['label'],\"SENT_NEGATIVE\")\n",
    "        self.assertEqual(sentiment_analyzer(\"I am neutral on Python\")['label'],\"SENT_NEUTRAL\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n",
    "\n",
    "#solution\n",
    "from SentimentAnalysis.sentiment_analysis import sentiment_analyzer\n",
    "import unittest\n",
    "\n",
    "class TestSentimentAnalyzer(unittest.TestCase):\n",
    "    def test_sentiment_analyzer(self):\n",
    "        result_1 = sentiment_analyzer('I love working with Python')\n",
    "        self.assertEqual(result_1['label'], 'SENT_POSITIVE')\n",
    "        result_2 = sentiment_analyzer('I hate working with Python')\n",
    "        self.assertEqual(result_2['label'], 'SENT_NEGATIVE')\n",
    "        result_3 = sentiment_analyzer('I am neutral on Python')\n",
    "        self.assertEqual(result_3['label'], 'SENT_NEUTRAL')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Deploy as web application using Flask\n",
    "Now that the application is ready, it is time to deploy it for usage over a web interface. To ease the process of deployment, you have been provided with 3 files which are going to be used for this task.\n",
    "\n",
    "1. `index.html` in `templates` folder.\n",
    "This file has the code for the web interface that has been designed for this lab. This is being provided for you in completion and is to be used as is. You are not required to make any changes to this file.\n",
    "\n",
    "The interface is as shown in the image.\n",
    "![image](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0224EN-Coursera/images/web_interface.png)\n",
    "\n",
    "2. `mywebscript.js` in static folder.\n",
    "Clicking the `Run Sentiment Analysis` button, on the html interface, calls this javascript file which executes a GET request and takes the text provided by the user as input. This text, saved in a variable named `textToAnalyze` is then passed on to the server file to be sent to the application. This file is also being provided to you in completion and is expectedc to be used as is. You are not required to make any changes to this file.\n",
    "\n",
    "3. `server.py` in the `practice_project` folder.\n",
    "This task revolves around the completion of this file. You can complete this file by completing the following 5 steps.\n",
    "\n",
    "**a.** Import the relevant libraries and functions\n",
    "\n",
    "In this file, you'll need the Flask library along with its `render_template` function (for deploying the HTML file) and `request` function (to initiate the GET request from the web page).\n",
    "\n",
    "You also would need to import the `sentiment_analyzer` function from the `SentimentAnalysis` package.\n",
    "\n",
    "Add the relevant lines of code, importing the said functions, in `server.py`.\n",
    "\n",
    "**b.** Initiate the Flask app by the name `Sentiment Analyzer`\n",
    "\n",
    "Put the knowledge gained in Module 2 of this course and add the statement to server.py, that initiates the application and names it `Sentiment Analyzer`.\n",
    "\n",
    "**c.** Define the function `sent_analyzer`\n",
    "\n",
    "The purpose of this function is two fold. First, the function should send a `GET` request to the HTML interface to receive the input text. Note that the `GET` request should reference `textToAnalyze` variable as defined in the `mywebscript.js` file. Store the incoming text to a variable `text_to_analyze`. Now, as the second function, call your `sentiment_analyzer` application with `text_to_analyze` as the argument.\n",
    "\n",
    "Also, format the returning output of the function in a formal text. For e.g.\n",
    "`The given text has been identified as POSITIVE with a score of 0.99765`.\n",
    "\n",
    "Note: The function uses the Flask decorator `@app.route(\"/sentimentAnalyzer\")` as referenced in the `mywebscript.js` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my code\n",
    "@app.route(\"/sentimentAnalyzer\")\n",
    "def sent_analyzer():\n",
    "    text_to_analyze = request.args.get('textToAnalyze')\n",
    "    \n",
    "    stored_text = sentiment_analyzer(text_to_analyze)\n",
    "    status = stored_text['documentSentiment']['label'].split(\"_\")[1]\n",
    "    score = stored_text['documentSentiment']['score']\n",
    "    \n",
    "    return f\"The given text has been identified as {status} with a score of {score}.\"\n",
    "\n",
    "\n",
    "# solution\n",
    "@app.route(\"/sentimentAnalyzer\")\n",
    "def sent_analyzer():\n",
    "    text_to_analyze = request.args.get('textToAnalyze')\n",
    "    response = sentiment_analyzer(text_to_analyze)\n",
    "    label = response['label']\n",
    "    score = response['score']\n",
    "    return \"The given text has been identified as {} with a score of {}.\".format(label.split('_')[1], score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Render the HTML template using `render_index_page`\n",
    "\n",
    "This function should simply run the render_template function on the HTML template, `index.html`.\n",
    "\n",
    "**e.** Run the application on localhost:5000\n",
    "\n",
    "Finally, upon file execution, run the application on host: `0.0.0.0` (or localhost) on port number 5000.\n",
    "\n",
    "##### To deploy the application, exectue the file server.py from the terminal.\n",
    "`python3.11 server.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: Incorporate Error handling\n",
    "To incorporate error handling, we need to identify the different forms of error codes that may be received in response to the GET query initiated by the `sent_analyzer` function in `server.py`.\n",
    "\n",
    "This is already a part of the Watson NLP Library functions and can be observed on the terminal console where the code is running.\n",
    "\n",
    "The codes indicate that the initial GET request was successful (200), the request was then successfully transferred to the Watson Library (304) and then the GET request to generate the response was also conducted successfully.\n",
    "\n",
    "In the case of invalid entries, the system responds with 500 error code, indicating that there is something wrong at the server end.\n",
    "Invalid entry could be anything that the model is not able to interpret. However, in the situation of this error, this application output doesn't get updated.\n",
    "\n",
    "![img](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0224EN-Coursera/images/Error_handling_1.png)\n",
    "\n",
    "Note that the output on the interface is the same as before, the text being analyzed is a random text and the Watson AI libraries are throwing a 500 error confirming that the model has not been able to process the request.\n",
    "\n",
    "To fix this bug in our application, we need to study the response received from the Watson AI library function, when the server generates 500 error. To test this, we need to retrace the steps taken in Task 2, and test the Watson AI library with an invalid string input.\n",
    "\n",
    "Open a python shell in the terminal and run the following commands to check the required output.\n",
    "\n",
    "\n",
    "```\n",
    "import requests\n",
    "url = \"https://sn-watson-sentiment-bert.labs.skills.network/v1/watson.runtime.nlp.v1/NlpService/SentimentPredict\"\n",
    "headers = {\"grpc-metadata-mm-model-id\": \"sentiment_aggregated-bert-workflow_lang_multi_stock\"}\n",
    "myobj = { \"raw_document\": { \"text\": \"as987da-6s2d aweadsa\" } }\n",
    "response = requests.post(url, json = myobj, headers=headers)\n",
    "print(response.status_code)\n",
    "\n",
    "myobj = { \"raw_document\": { \"text\": \"Testing this application for error handling\" } }\n",
    "response = requests.post(url, json = myobj, headers=headers)\n",
    "print(response.status_code)\n",
    "```\n",
    "\n",
    "The console response looks as shown in the image below. The red boxes indicate the invalid text and its status code received, and the yellow boxes indicate the valid text and its status code received.\n",
    "\n",
    "![img](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-PY0224EN-Coursera/images/Error_handling_2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This enables you to modify the application in such a fashion, that we can send different outputs for different status codes.\n",
    "\n",
    "In the first part of this task, you have to modify the `sentiment_analyzer()` function to return the both `label` and `score` as `None` in case of invalid text entry.\n",
    "\n",
    "Now, in `server.py`, the response to be sent to the console should also be different for the valid and invalid input types.\n",
    "\n",
    "For invalid input, let the console print `Invalid input ! Try again`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my code\n",
    "@app.route(\"/sentimentAnalyzer\")\n",
    "def sent_analyzer():\n",
    "    text_to_analyze = request.args.get('textToAnalyze')\n",
    "    stored_text = sentiment_analyzer(text_to_analyze)\n",
    "    \n",
    "    if stored_text['label'] is not None:\n",
    "        status = stored_text['label'].split(\"_\")[1]\n",
    "        score = stored_text['score']\n",
    "    \n",
    "        return f\"The given text has been identified as {status} with a score of {score}.\"\n",
    "    \n",
    "    else:\n",
    "        return \"Invalid input ! Try again.\"\n",
    "\n",
    "# solution\n",
    "def sent_analyzer():\n",
    "    text_to_analyze = request.args.get('textToAnalyze')\n",
    "    response = sentiment_analyzer(text_to_analyze)\n",
    "    label = response['label']\n",
    "    score = response['score']\n",
    "    if label is None:\n",
    "        return \"Invalid input ! Try again.\"\n",
    "    else:\n",
    "        return \"The given text has been identified as {} with a score of {}.\".format(label.split('_')[1], score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
